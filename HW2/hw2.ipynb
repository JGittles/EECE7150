{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "# import tkinter\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "# matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib tk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the pointLoad function loads in the specified image and prompts to hand pick a set amount of points. TODO: store points for future runs to disable popup's and embed images for ease of documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointLoad(imgpath,pts = 0, imgpass = True):\n",
    "\n",
    "    if os.path.exists(imgpath):\n",
    "        img = cv2.imread(imgpath)\n",
    "        img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        if pts:\n",
    "            plt.imshow(img)\n",
    "            ptMatrix = plt.ginput(pts)\n",
    "            # plt.close()\n",
    "            if imgpass:\n",
    "                return img, np.array(ptMatrix).astype(int)\n",
    "            else:\n",
    "                return np.array(ptMatrix).astype(int)    \n",
    "        else:\n",
    "            return img\n",
    "    else:\n",
    "        print(\"Invalid image path\")\n",
    "        return\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getH creates a homography matrix between the source and destination photos, instead of using svd, I opted to construct the matrix with an explicit final h33 param = 1 and use least squares, something svd does internally anyway but I want to be difficult..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getH(dest, src):\n",
    "    p = np.array([0,0,0,0,0,0,0,0,1])\n",
    "    # if len(dest) < 5 and len(src) < 5:\n",
    "        \n",
    "\n",
    "    for i in range(len(dest)):\n",
    "        pi = np.array([[-src[i][0],-src[i][1],-1,0,0,0,src[i][0]*dest[i][0],src[i][1]*dest[i][0],dest[i][0]],\n",
    "        [0,0,0,-src[i][0],-src[i][1],-1,src[i][0]*dest[i][1],src[i][1]*dest[i][1],dest[i][1]]])\n",
    "        p = np.vstack((pi,p))\n",
    "\n",
    "  \n",
    "    x = np.zeros((len(dest)*2,1))\n",
    "    x = np.vstack((x,np.array([1])))\n",
    "    H, residuals, rank, s = np.linalg.lstsq(p,x)\n",
    "    H = H.reshape((3,3))\n",
    "    \n",
    "\n",
    "    \n",
    "    return H\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pano is a vestigiel function from when I thought pt1 of this homework let you use a detector for finding points on the panorama =( I'm leaving it here to use for pt2 so not all is lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pano(image_paths):\n",
    "    imgs = []\n",
    "    imgs_gray = []\n",
    "    \n",
    "    orbvals={\"keypts\":[],\"descrpt\":[],\"matches\":[]}\n",
    "    orb = cv2.ORB_create()\n",
    "    # sift = cv2.xfeatures2d.SIFT_create(nfeatures=200)\n",
    "    for i in range(len(image_paths)):\n",
    "        imgs.append(cv2.imread(image_paths[i]))\n",
    "        imgs[i]=cv2.resize(imgs[i],(0,0),fx=0.4,fy=0.4)\n",
    "        imgs_gray.append(cv2.cvtColor(imgs[i],cv2.COLOR_BGR2GRAY))\n",
    "        keypt, desc = orb.detectAndCompute(imgs_gray[i],None)\n",
    "        # keypt, desc = sift.detectAndCompute(imgs_gray[i], None)\n",
    "        orbvals[\"keypts\"].append(keypt)\n",
    "        orbvals[\"descrpt\"].append(desc)\n",
    "        # print((orbvals[\"keypts\"]))\n",
    "        # cv2.imshow(\"image\",cv2.drawKeypoints(imgs[i], orbvals[\"keypts\"][i], None, (255, 0, 255)))\n",
    "\n",
    "\n",
    "    bf = cv2.BFMatcher_create(cv2.NORM_HAMMING)    \n",
    "    for i in range(len(imgs)-1):\n",
    "        orbvals[\"matches\"].append(bf.knnMatch(orbvals[\"descrpt\"][i], orbvals[\"descrpt\"][i+1],k=2))\n",
    "    print(orbvals['matches'])\n",
    "    cv2.imshow(draw_matches(imgs[0],orbvals[\"keypts\"][0],imgs[1],orbvals[\"keypts\"][1],orbvals[\"matches\"][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "draw_matches function to visualize points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_matches(img1, keypoints1, img2, keypoints2, matches):\n",
    "  r, c = img1.shape[:2]\n",
    "  r1, c1 = img2.shape[:2]\n",
    "\n",
    "  # Create a blank image with the size of the first image + second image\n",
    "  output_img = np.zeros((max([r, r1]), c+c1, 9), dtype='uint8')\n",
    "  output_img[:r, :c, :] = np.dstack([img1, img1, img1])\n",
    "  output_img[:r1, c:c+c1, :] = np.dstack([img2, img2, img2])\n",
    "\n",
    "  # Go over all of the matching points and extract them\n",
    "  for match in matches:\n",
    "    (x1, y1) = keypoints1[img1_idx].pt\n",
    "    (x2, y2) = keypoints2[img2_idx].pt\n",
    "\n",
    "    # Draw circles on the keypoints\n",
    "    cv2.circle(output_img, (int(x1),int(y1)), 4, (0, 255, 255), 1)\n",
    "    cv2.circle(output_img, (int(x2)+c,int(y2)), 4, (0, 255, 255), 1)\n",
    "\n",
    "    # Connect the same keypoints\n",
    "    cv2.line(output_img, (int(x1),int(y1)), (int(x2)+c,int(y2)), (0, 255, 255), 1)\n",
    "    \n",
    "  return output_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to paste the source image onto the destination, the implementation itself is a little hack-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pasteImages(img1,img1pts,img2,img2pts):\n",
    "    H = getH(img1pts,img2pts)\n",
    "    im_out = cv2.warpPerspective(img2,H,(img1.shape[1],img1.shape[0]))\n",
    "    mask = np.zeros(img1.shape[:2], dtype=\"uint8\")\n",
    "    cv2.fillConvexPoly(mask, np.int32([img1pts]),color=255)\n",
    "    cv2.fillConvexPoly(img1, np.int32([img1pts]),color=0)\n",
    "    masked =cv2.bitwise_and(im_out,im_out,mask=mask)\n",
    "    img3 = cv2.add(img1,masked)\n",
    "    # img3=cv2.cvtColor(img3, cv2.COLOR_BGR2RGB)\n",
    "    return img3, H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract 4 points from the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1, img1pts = pointLoad(\"image1.jpg\",4)\n",
    "\n",
    "img2, img2pts = pointLoad(\"image2.jpg\",4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paste the images together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img3, H3 = pasteImages(img1,img1pts,img2,img2pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and display..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(img3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process can also be repeated with as many points as youd like due to the getH function performing least squares on the images, here we will do just that with 8 points from each image..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1New, img1ptsNew = pointLoad(\"image1.jpg\",8)\n",
    "img2New, img2ptsNew = pointLoad(\"image2.jpg\",8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img3New, H3New = pasteImages(img1New,img1ptsNew,img2New,img2ptsNew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img3New)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can compare the two H matrix's and see how they differ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(H3)\n",
    "print(\"~~~~\")\n",
    "print(H3New)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, both Homographies differ while still yielding similar transoformations. In part, this is due to my bad clicking accuracy between images. A more major factor however, is that the transofrms are scaled differently.\n",
    "If you observe the value at h33, the scale factors differ wildly, given that that h33 = wk where w is an arbitrary scale factor and k is 1. This difference for us does not matter fortunately as these transformations are scale invariant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets perform this 8 point homography again but for a new image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img4, img4pts = pointLoad(\"image3.jpg\",8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img5, H5 = pasteImages(img1New,img1ptsNew,img4,img4pts)\n",
    "\n",
    "plt.imshow(img5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try some panorama stitching. This will work in much the same way as before but with some extra complications for our final output image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_paths=['1.jpg','2.jpg','3.jpg','4.jpg','5.jpg']\n",
    "# left={'pts':[],'h':[]}\n",
    "# right={'pts':[],'h':[]}\n",
    "# # get points/homographies for left-bound images\n",
    "# for n in [2,1]:\n",
    "#     destpts = pointLoad(image_paths[n],4,False)\n",
    "#     left['pts'].append(destpts)\n",
    "#     srcpts = pointLoad(image_paths[n-1],4,False)\n",
    "#     left['pts'].append(srcpts)\n",
    "#     left['h'].append(getH(destpts,srcpts))\n",
    "\n",
    "# for n in [2,3]:\n",
    "#     destpts = pointLoad(image_paths[n],4,False)\n",
    "#     right['pts'].append(destpts)\n",
    "#     srcpts = pointLoad(image_paths[n+1],4,False)\n",
    "#     right['pts'].append(srcpts)\n",
    "#     right['h'].append(getH(destpts,srcpts))\n",
    "    \n",
    "\n",
    "\n",
    "left= {'pts':[np.array([[ 66, 333],\n",
    "       [119, 330],\n",
    "       [124, 428],\n",
    "       [ 68, 434]]), np.array([[568, 361],\n",
    "       [611, 353],\n",
    "       [616, 446],\n",
    "       [569, 452]]), np.array([[344, 456],\n",
    "       [487, 439],\n",
    "       [488, 488],\n",
    "       [347, 499]]), np.array([[750, 436],\n",
    "       [899, 410],\n",
    "       [900, 463],\n",
    "       [751, 480]])], 'h': [np.array([[ 1.09302538e+01,  2.20491068e-01, -5.76948603e+03],\n",
    "       [ 4.60920510e+00,  9.74051812e+00, -3.51831044e+03],\n",
    "       [ 1.05745646e-02,  2.35356950e-03,  1.00000000e+00]]), np.array([[ 2.26918546e+00,  2.88756032e-01, -1.16863724e+03],\n",
    "       [ 5.01158722e-01,  2.16272647e+00, -4.45061492e+02],\n",
    "       [ 8.81876514e-04,  5.84231271e-04,  1.00000000e+00]])]}\n",
    "right={'pts': [np.array([[ 993,  297],\n",
    "       [1057,  292],\n",
    "       [1059,  385],\n",
    "       [ 994,  388]]), np.array([[481, 376],\n",
    "       [535, 375],\n",
    "       [539, 461],\n",
    "       [483, 461]]), np.array([[633, 378],\n",
    "       [171, 458]])], 'h': [np.array([[ 9.87533238e-01,  2.33782564e-01,  4.33166039e+02],\n",
    "       [-1.27974513e-01,  1.17262990e+00, -8.14343553e+01],\n",
    "       [-1.89906519e-04,  2.51166116e-04,  1.00000000e+00]]), np.array([[ 3.23123829e-01, -2.92838171e-01,  5.31994939e+02],\n",
    "       [-2.61739097e-01,  5.04467128e-01,  1.44761482e+02],\n",
    "       [-4.77680542e-04, -4.37126071e-04,  1.00000000e+00]])]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an array of the left homographies and of the right as well as points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pts': [array([[ 66, 333],\n",
      "       [119, 330],\n",
      "       [124, 428],\n",
      "       [ 68, 434]]), array([[568, 361],\n",
      "       [611, 353],\n",
      "       [616, 446],\n",
      "       [569, 452]]), array([[344, 456],\n",
      "       [487, 439],\n",
      "       [488, 488],\n",
      "       [347, 499]]), array([[750, 436],\n",
      "       [899, 410],\n",
      "       [900, 463],\n",
      "       [751, 480]])], 'h': [array([[ 1.09302538e+01,  2.20491068e-01, -5.76948603e+03],\n",
      "       [ 4.60920510e+00,  9.74051812e+00, -3.51831044e+03],\n",
      "       [ 1.05745646e-02,  2.35356950e-03,  1.00000000e+00]]), array([[ 2.26918546e+00,  2.88756032e-01, -1.16863724e+03],\n",
      "       [ 5.01158722e-01,  2.16272647e+00, -4.45061492e+02],\n",
      "       [ 8.81876514e-04,  5.84231271e-04,  1.00000000e+00]])]}\n",
      "~~~\n",
      "{'pts': [array([[ 993,  297],\n",
      "       [1057,  292],\n",
      "       [1059,  385],\n",
      "       [ 994,  388]]), array([[481, 376],\n",
      "       [535, 375],\n",
      "       [539, 461],\n",
      "       [483, 461]]), array([[633, 378],\n",
      "       [171, 458]])], 'h': [array([[ 9.87533238e-01,  2.33782564e-01,  4.33166039e+02],\n",
      "       [-1.27974513e-01,  1.17262990e+00, -8.14343553e+01],\n",
      "       [-1.89906519e-04,  2.51166116e-04,  1.00000000e+00]]), array([[ 3.23123829e-01, -2.92838171e-01,  5.31994939e+02],\n",
      "       [-2.61739097e-01,  5.04467128e-01,  1.44761482e+02],\n",
      "       [-4.77680542e-04, -4.37126071e-04,  1.00000000e+00]])]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(left)\n",
    "print('~~~')\n",
    "print(right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do any transforms, we need to relate our outer edges of the panorama to the central image (image 3). this could either be done by warping the outermost image to the one inner image and then warping those to the center.\n",
    "This method, however, is flawed as it will result in some drift from the error in the homographies.\n",
    "\n",
    "A better way to apraoch the edge images is to dot the outer homoography with the inner to create a direct relation from the outer image to the central one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=[]\n",
    "for image in image_paths:\n",
    "    img = (pointLoad(image))\n",
    "    cv2.imshow(\"image\",img)\n",
    "    images.append(img)\n",
    "\n",
    "# np.zeros()\n",
    "# im_out = cv2.warpPerspective(images[1],left['h'][0],(5000,5000))\n",
    "# cv2.imshow('warped',im_out)\n",
    "# # cv2.addweighted(orig1, alpha, orig2, (1- alpha), 0, output);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f32c342d9ea7f5036db09204dae45f5252a65bc883a367b7eae2cb473c9503d1"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('afr': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
